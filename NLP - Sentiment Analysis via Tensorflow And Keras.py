# -*- coding: utf-8 -*-
"""Tensorflow and Keras.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F1UXOPjDrr2LBUTzuesezeTNNK2Ygh7H
"""

#Tensorflow and Keras
#Classifier and Sarcasm Detection -> NLP, Keras and Tensorflow
#Embedding layer, MaxPoolLayer, Layer Network

#Problem Statement:
#Sentence -> detect sarcastic or not?
#Input -> Sentence -> text ->
#I love a cat -> sarcastic = 1 || not sarcastic = 0
#I love a cat -> Its a english language which computer doesn't understands

#Use Tokenizer
# Convert words into a list of tokens.
# This is done first during Natural Language Processing.
# I love my dog
#[1  2   3  4]
# I love my cat
#[1  2   3  5]

#[1  2   3  4]
#[1  2   3  5]
# There is similarity

# tensorflow is lib by Google for ML and DL
from tensorflow.keras.preprocessing.text import Tokenizer
sentences = [
    'i love my dog',
    'I, love my cat',
    'You love my dog!'
]
tokenizer = Tokenizer(num_words = 100)#create tokenizer
tokenizer = Tokenizer(oov_token = "<OOV>")
#or tokenizer = Tokenizer(num_words = 100, oov_token = "<OOV>")#create tokenizer with oov_token
tokenizer.fit_on_texts(sentences)#create tokenizer for given sentences via Bag of Words
word_index = tokenizer.word_index
print(word_index)
#i or I are changed to i(lowercase) and , and ! are removed
#[i love my dog] -> [3 1 2 4] -> this can be an input to my neural network

#Problem 1 is -> Out of vocab words can be handled by oov_token.
#I love your dog -> [3, 1, 4] -> your is not there
#My tokenizer is fitted on different sentences
#To avoid this i can use oov_token -> Out of vocabulary token
#I love your dog -> [3, 1, 10, 4]

#Problem 2 is -> When training a ML model, all sentences should be of equal length
#Solved using Padding -> maximum_length = 8
#[I love your dog] -> [0 0 0 0 3 1 2 4] or [3 1 2 4 0 0 0 0] -> Padding can be done in front or end.
#[This is it] -> [7 8 9 0 0 0 0 0]

from typing import Sequence
# tensorflow is lib by Google for ML and DL
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
sentences = [
    'i love my dog',
    'I, love my cat',
    'You love my dog!',
    'Do you think my dog is amazing?'
]
tokenizer = Tokenizer(num_words = 100, oov_token = "<OOV>")
#or tokenizer = Tokenizer(num_words = 100, oov_token = "<OOV>")#create tokenizer with oov_token
tokenizer.fit_on_texts(sentences)#create tokenizer for given sentences via Bag of Words
word_index = tokenizer.word_index
print(word_index)

sequences = tokenizer.texts_to_sequences(sentences) #converts words to numbers
print(sequences)
#MY ML model can only take sentences of same length
padded = pad_sequences(sequences, maxlen = 5, truncating = 'post') #default truncating is pre
print("\nPadded Sequences:")
print(padded)
#[ 0  5  3  2  4] first 1 number is padded with 0
#[ 9  2  4 10 11] -> first 2 numbers are removed
# default padding or removing happens from start

padded = pad_sequences(sequences)
print("\nPadded Sequences:")
print(padded)

#Try with words that tokenizer was not fit to
test_data = [
    'i really love my dog',
    'my dog loves my manatee'
]
# this will be converted to:
#test_data = [
#     'i <oov> love my dog',
#     'my dog loves my <oov>'
# ]

test_seq = tokenizer.texts_to_sequences(test_data)
print("Test Sequence = ", test_seq)
#really, loves, manatee is given as 1

padded = pad_sequences(test_seq, maxlen = 10, padding = 'post') #default padding is pre
print("\nPadded Sequences:")
print(padded)

# Commented out IPython magic to ensure Python compatibility.
# Run this to ensure TensorFlow 2.x is used
try:
  # %tensorflow_version only exists in Colab.
#   %tensorflow_version 2.x
except Exception:
  pass

#Sarcasm detection
import json
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

vocab_size = 10000 #large vocab with 10000 most frequent words
max_length = 100 #max length of sentence
oov_tok = "<OOV>"
trunc_type='post'
padding_type='post'
training_size = 20000
embedding_dim = 16

#!wget "https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection?select=Sarcasm_Headlines_Dataset.json" -O sarcasm.json

!wget --no-check-certificate \
    https://storage.googleapis.com/learning-datasets/sarcasm.json \
    -O /tmp/sarcasm.json

with open("/tmp/sarcasm.json", 'r') as f:
  datastore = json.load(f)

sentences = []
labels = []


for item in datastore:
    sentences.append(item['headline'])
    labels.append(item['is_sarcastic'])

print(sentences[0])
print(labels[0])

print(sentences[2])
print(labels[2])

print(len(sentences)) #26709

print (len(sentences))
print (len(labels))

#Train and test imports separately
#Train ML model for 20000 sentences and test for 7000 sentences

#label 1 is sarcastic and 0 is not sarcastic
print(training_size)
training_sentences = sentences[0:training_size]
testing_sentences = sentences[training_size:]
training_labels = labels[0:training_size]
testing_labels = labels[training_size:]

tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(training_sentences)

word_index = tokenizer.word_index

training_sequences = tokenizer.texts_to_sequences(training_sentences)
training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)

tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(testing_sentences)

testing_sequences = tokenizer.texts_to_sequences(testing_sentences)
testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)

print (training_padded[0])

print (testing_padded[0])

# Need this block to get it to work with TensorFlow 2.x
import numpy as np
training_padded = np.array(training_padded)
training_labels = np.array(training_labels)
testing_padded = np.array(testing_padded)
testing_labels = np.array(testing_labels)

print(training_padded)

print(testing_padded)

print(testing_labels)

print(training_labels)

#Now data is ready to go into a model
# model = tf.keras.Sequential([
#     tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length),#words are represented in 100 * 16 dimensional vector
#     tf.keras.layers.GlobalAveragePooling1D(),
    #converts into 1*16 horizontal layer
    #convert into vertical layer and pass it to neural network
    # tf.keras.layers.Dense(24, activation = 'relu'),
    #24 neurons
    # tf.keras.layers.Dense(1, activation = 'sigmoid')
    #1 -> neuron and probabilistic distribuition is 1
    #can replace dense layer with RNN or LSTM
# ])

#vector sequences using an emdedding layer
#bad(-1,0),not good(-1,1) <--|--> good(1,0), not bad(1,1)
#word2vec

# model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])


model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(24, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

model.summary()
#100*16 is shape of embedding layer

# num_epochs = 30
#30 times i want to iterate over datasets
# history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)
#verbose -> how many data i want to print on screen
#accuracy will increase with every epoch


num_epochs = 30
history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)

import matplotlib.pyplot as plt


def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()

plot_graphs(history, "accuracy")
plot_graphs(history, "loss")

#training accuracy will always increase
#validation accuracy will increase, stabilise then it might decrease

#if epoch is 100, then overfitting happens,

sentence = ["granny starting to fear spiders in the garden might be real", "game of thrones season finale showing this sunday night"]
sequences = tokenizer.texts_to_sequences(sentence)
padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)
print(model.predict(padded))
#9.91e-01 -> 0.91 so sarcastic
#5.85e-06 -> 0.00000058 so not sarcastic

